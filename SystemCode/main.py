# -*- coding: utf-8 -*-
"""nus_project1_2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rPfMlFK5Rr-hAIsd4z0ux3ewZ7ds0fTh
"""

import pandas as pd
import numpy as np
import torch
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import SAGEConv
from torch_geometric.loader import NeighborLoader
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import networkx as nx
import matplotlib.pyplot as plt
from tqdm import tqdm
import json
import os
from itertools import combinations
from apriori_python import apriori



def load_data(path='data/Marker_Basket_Optimisation.csv'):
    data = pd.read_csv(path, header=None)
    print("data.shape:", data.shape)
    return data


def split_data(data, test_size=0.2):
    train_df, test_df = train_test_split(data, test_size=test_size, random_state=42)
    print("After split, trainingData:", train_df.size, "testData:", test_df.size)
    return train_df, test_df


def analyze_basic_stats(data):
    """
    计算每个商品的出现频率
    data：DataFrame，包含商品信息
    """

    # 创建一个空字典保存商品出现次数
    item_frequencies = {}
    count = 0
    # 遍历 DataFrame 每一行
    for index, row in data.iterrows():
        count += 1
        for item in row:
            if pd.isna(item):
                continue
            if item in item_frequencies:
                item_frequencies[item] += 1
            else:
                item_frequencies[item] = 1
    # 按出现次数排序
    item_frequencies = dict(sorted(item_frequencies.items(), key=lambda x: x[1], reverse=True))
    print("Number of transaction:", count)
    print('商品种类:', len(item_frequencies))
    print("每种商品出现次数：")
    for item, count in item_frequencies.items():
        print(f"{item}: {count}")
    return item_frequencies


def pre_process(data):
    """
    extract transactions from raw data
    :param data:
    :return: transactions
    """
    transactions = []
    for index, row in data.iterrows():
        # 移除NaN值、空字符串并转换为字符串
        items = [str(item).strip() for item in row.values if pd.notna(item) and str(item).strip()]
        if len(items) >= 2:  # 只保留至少包含2个商品的交易
            transactions.append(items)

    total_transactions = len(transactions)
    print(f"Total number of transactions: {total_transactions}")
    return transactions


def itemcounts(itemsets):
    """
    :param itemsets:
    :return: allitems_count
    """
    allitems_count = dict()
    for its in itemsets:
        for i in its:
            if i in allitems_count.keys():
                allitems_count[i] = allitems_count[i] + 1
            else:
                allitems_count[i] = 1
    return allitems_count


def display_as_barchart(sorted_itemfreqcnts, topN=20):
    rankeditems = [k for k, v in sorted_itemfreqcnts]
    frequencies = [v for k, v in sorted_itemfreqcnts]
    plt.barh(rankeditems[0:topN], frequencies[0:topN], align='center', alpha=0.5)


def extract_allitems_set(transactions):
    """
    extract All_items_set
    :param transactions:
    :return:
    """
    All_items_set = set()
    for transaction in transactions:
        for item in transaction:
            All_items_set.add(item)
    return All_items_set


def label_allitems(all_items):
    le = LabelEncoder()
    All_items_encoded = le.fit_transform(list(all_items))
    print(All_items_encoded)
    print(le.classes_)
    return All_items_encoded


def rules_from_apriori(transactions):
    freqItemSet, rules = apriori(transactions, minSup=0.02, minConf=0.1)
    print(len(rules))
    print(rules)
    return freqItemSet, rules


def build_graph(transactions, All_items, item_frequencies, min_support=0.01, ):
    """
    Optimize co-occurrence graph construction by dynamically adjusting the support threshold.
    :param transactions:
    :return: graph
    """
    if transactions is None:
        raise ValueError("请先加载数据")

    print("构建商品共现图...")

    item_indices = {item: i for i, item in enumerate(All_items)}
    num_items = len(All_items)
    co_occurrence = np.zeros((num_items, num_items), dtype=np.int32)

    # 计算共现次数
    for transaction in tqdm(transactions, desc="处理交易"):
        if len(transaction) < 2:
            continue
        indices = [item_indices[item] for item in transaction]
        # 优化共现计算效率
        for i, j in combinations(indices, 2):
            co_occurrence[i, j] += 1
            co_occurrence[j, i] += 1

    total_transactions = len(transactions)
    edges = []
    edge_weights = []

    # Dynamically adjust the support threshold: if no rules are found under the initial threshold, automatically lower it.

    while len(edges) == 0 and min_support > 0:
        edges = []
        edge_weights = []
        for i in range(num_items):
            for j in range(i + 1, num_items):
                support = co_occurrence[i, j] / total_transactions
                if support >= min_support:
                    edges.append((i, j))
                    edge_weights.append(support)
        if len(edges) == 0:
            min_support *= 0.5  # 降低阈值
            print(f"支持度 {min_support} 下无共现关系，尝试降低至 {min_support}")

    if len(edges) == 0:
        print("警告：即使降低支持度阈值，仍未发现共现关系")
    else:
        print(f"edges: {len(edges)} with support >= {min_support}")

    # 创建PyTorch Geometric数据对象
    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
    edge_weight = torch.tensor(edge_weights, dtype=torch.float)

    # 为每个节点创建特征：使用商品频率作为初始特征
    item_freq = [0] * num_items
    for item, count in item_frequencies.items():
        item = item.strip()
        item_freq[item_indices[item]] = count / total_transactions

    x = torch.tensor(item_freq, dtype=torch.float).view(-1, 1)  # 转换为二维张量

    # 创建图数据对象
    graph = Data(x=x, edge_index=edge_index, edge_weight=edge_weight)
    graph.num_nodes = num_items

    return graph


def build_model(graph, embedding_dim=32, hidden_dim=64):
    """构建GraphSAGE模型"""
    if graph is None:
        raise ValueError("请先构建图")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # 定义GraphSAGE模型
    class GraphSAGE(torch.nn.Module):
        def __init__(self, in_channels, hidden_channels, out_channels):
            super().__init__()
            self.conv1 = SAGEConv(in_channels, hidden_channels)
            self.conv2 = SAGEConv(hidden_channels, out_channels)

        def forward(self, x, edge_index):
            x = self.conv1(x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, p=0.2, training=self.training)
            x = self.conv2(x, edge_index)
            return x

    # 初始化模型
    model = GraphSAGE(
        in_channels=graph.x.size(1),
        hidden_channels=hidden_dim,
        out_channels=embedding_dim
    ).to(device)

    print(f"GraphSAGE模型构建完成，设备: {device}")
    return model

def train_model(model,graph, epochs=100, lr=0.01, batch_size=512):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    """训练GraphSAGE模型"""
    if model is None:
        build_model()

    if graph is None:
        raise ValueError("请先构建图")

    print(f"开始训练模型，共 {epochs} 轮...")
    print(f"图中节点数量: {graph.num_nodes}")  # 新增：打印节点数量，用于调试

    # 定义损失函数和优化器
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # 创建邻居加载器用于小批量训练
    loader = NeighborLoader(
        graph,
        num_neighbors=[10, 10],  # 每层采样的邻居数量
        batch_size=batch_size,
        shuffle=True,
    )

    # 训练循环
    model.train()
    losses = []

    for epoch in range(epochs):
        total_loss = 0
        for batch in loader:
            batch = batch.to(device)
            optimizer.zero_grad()

            # 前向传播
            out = model(batch.x, batch.edge_index)

            # 计算对比损失 - 相似节点应该有相似的嵌入
            # 这里使用边连接的节点作为正样本
            edge_index = batch.edge_index
            src, dst = edge_index[0], edge_index[1]

            # 确保src和dst索引在有效范围内
            max_index = out.size(0) - 1
            if src.max() > max_index or dst.max() > max_index:
                print(f"警告：发现超出范围的索引，src_max={src.max()}, dst_max={dst.max()}, 最大允许索引={max_index}")
                # 过滤掉超出范围的索引
                valid_mask = (src <= max_index) & (dst <= max_index)
                src = src[valid_mask]
                dst = dst[valid_mask]
                if len(src) == 0:
                    continue  # 如果没有有效索引，跳过本轮

            # 计算正样本对的相似度
            pos_sim = F.cosine_similarity(out[src], out[dst], dim=1)

            # 随机生成负样本对 - 修复部分
            # 确保负样本索引在有效范围内
            if out.size(0) == 0:
                continue  # 防止空数据导致错误

            # 生成有效范围内的负样本
            neg_dst = torch.randint(0, out.size(0), (src.size(0),), device=device)

            # 计算负样本对的相似度
            neg_sim = F.cosine_similarity(out[src], out[neg_dst], dim=1)

            # 对比损失
            loss = -torch.log(torch.sigmoid(pos_sim) + 1e-10).mean() - torch.log(
                1 - torch.sigmoid(neg_sim) + 1e-10).mean()

            # 反向传播和优化
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(loader)
        losses.append(avg_loss)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}")

    # 绘制损失曲线
    # plt.figure(figsize=(10, 6))
    # plt.plot(range(1, epochs + 1), losses)
    # plt.title('Training Loss')
    # plt.xlabel('Epoch')
    # plt.ylabel('Loss')
    # plt.grid(True)
    # plt.savefig('training_loss.png')
    # plt.close()

    return model


def rules_from_GNN(model,graph,transactions,All_items_set, top_k=150):
    """从训练好的模型中提取关联规则，输出为 [[{A}, {B}, value], ...] 格式（JSON可序列化）"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    item_encoder = LabelEncoder()
    item_encoder.fit(list(All_items_set))
    if model is None:
        raise ValueError("请先训练模型")

    model.eval()
    with torch.no_grad():
        embeddings = model(graph.x.to(device), graph.edge_index.to(device))

    similarities = F.cosine_similarity(embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=2)

    rules = []
    item_list = item_encoder.classes_
    num_items = len(item_list)

    for i in range(num_items):
        for j in range(num_items):
            if i != j:
                support = (
                    graph.edge_weight[
                        (graph.edge_index[0] == i) & (graph.edge_index[1] == j)
                        ].sum().item() / len(transactions)
                    if len(graph.edge_weight) > 0
                    else 0
                )
                confidence = similarities[i][j].item()

                if confidence > 0.5 and support > 0:
                    rules.append([
                        [str(item_list[i]).split()],  # 集合改为列表，避免JSON序列化报错
                        [str(item_list[j]).split()],
                        confidence
                    ])

    # 排序并取前 top_k
    rules = sorted(rules, key=lambda x: x[2], reverse=True)[:top_k]

    # 保存为 JSON
    with open("results/rules.json", "w", encoding="utf-8") as f:
        json.dump(rules, f, ensure_ascii=False, indent=2)

    print(f"已提取 {len(rules)} 条规则并保存到 results/rules.json")
    print(rules)
    return rules


def extract_rules(method):
    '''

    :param method: "apriori","GNN"
    :return: rules
    '''
    path = 'data/Market_Basket_Optimisation.csv'
    data = load_data(path)

    train_df, test_df = split_data(data)

    item_frequencies = analyze_basic_stats(train_df)
    print('item_frequencies',item_frequencies)
    transactions = pre_process(train_df)


    if method == 'apriori':
        freqItemSet, rules = rules_from_apriori(transactions)

    elif method == 'GNN':
        All_items_set = extract_allitems_set(transactions)
        All_items_labeled = label_allitems(All_items_set)
        graph = build_graph(transactions,All_items_set,item_frequencies,min_support=0.01)
        model = build_model(graph)
        model = train_model(model,graph)
        rules = rules_from_GNN(model,graph,transactions,All_items_set)


from collections import deque, defaultdict
from typing import List, Tuple, Dict, Set, Optional, Any, Iterable
import random
import math
import json
from pathlib import Path

# =========================
# Basic types & constants
# =========================
Coord = Tuple[int, int]            # (row, col)
Grid  = List[List[int]]            # 0 = aisle, 1 = shelf

MAX_ITEMS_PER_SHELF = 4

# =========================
# Grid / aisle utilities
# =========================
DIRS = [(1,0),(-1,0),(0,1),(0,-1)]

def in_bounds(r: int, c: int, H: int, W: int) -> bool:
    return 0 <= r < H and 0 <= c < W

def build_grid_from_shelves(H: int, W: int, shelves: List[Coord]) -> Grid:
    """Create a grid from shelf coordinates: 0=aisle, 1=shelf."""
    g = [[0]*W for _ in range(H)]
    for r, c in shelves:
        if not in_bounds(r, c, H, W):
            raise ValueError(f"Shelf {(r,c)} out of bounds for {H}x{W} grid.")
        g[r][c] = 1
    return g

def is_aisle(grid: Grid, rc: Coord) -> bool:
    r, c = rc
    return grid[r][c] == 0

def neighbors4(grid: Grid, rc: Coord):
    r, c = rc
    H, W = len(grid), len(grid[0])
    for dr, dc in DIRS:
        nr, nc = r+dr, c+dc
        if in_bounds(nr, nc, H, W):
            yield (nr, nc)

def any_adjacent_aisles(grid: Grid, shelf: Coord) -> Set[Coord]:
    """Aisle cells adjacent to a given shelf cell."""
    out = set()
    for nb in neighbors4(grid, shelf):
        if is_aisle(grid, nb):
            out.add(nb)
    return out

def reachable_aisles_from_start(grid: Grid, start: Coord) -> Set[Coord]:
    """BFS over aisles; return all aisle cells reachable from 'start'."""
    if not is_aisle(grid, start):
        return set()
    q = deque([start])
    vis = {start}
    while q:
        u = q.popleft()
        for v in neighbors4(grid, u):
            if is_aisle(grid, v) and v not in vis:
                vis.add(v)
                q.append(v)
    return vis

# =========================
# Rules loader (JSON → internal format)
# =========================
def _is_list_of_str(x):
    return isinstance(x, list) and all(isinstance(t, str) for t in x)

def _flatten_to_items(side):
    """
    Turn nested token lists into item-name strings.
    Examples:
      ["fresh","tuna"]        -> ["fresh tuna"]
      [[["fresh","tuna"]]]   -> ["fresh tuna"]
      [["ham"],["oil"]]      -> ["ham","oil"]
      "chicken"              -> ["chicken"]
    """
    if isinstance(side, str):
        return [side]
    if _is_list_of_str(side):
        return [" ".join(side)]
    items = []
    if isinstance(side, list):
        for e in side:
            items.extend(_flatten_to_items(e))
    return [s for s in items if isinstance(s, str) and s.strip()]

def load_rules_from_json(json_path: str):
    """
    Load rules from a JSON file placed in the same directory as the notebook/script.
    Output format: list of [ set(A), set(B), weight ]
    Accepts entries like:
      [ <nested tokens for A>, <nested tokens for B>, weight ]
      or dicts with keys: 'antecedent', 'consequent', plus 'lift'/'confidence'/'support'
    """
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    rules = []
    for r in data:
        if isinstance(r, dict):
            A_raw = r.get("antecedent", [])
            B_raw = r.get("consequent", [])
            w = r.get("lift", None)
            if w is None:
                w = r.get("confidence", None)
            if w is None:
                w = r.get("support", 1.0)
            A = _flatten_to_items(A_raw)
            B = _flatten_to_items(B_raw)
            if A and B:
                rules.append([set(A), set(B), float(w)])
        elif isinstance(r, (list, tuple)) and len(r) >= 2:
            A = _flatten_to_items(r[0])
            B = _flatten_to_items(r[1])
            w = float(r[2]) if len(r) >= 3 and isinstance(r[2], (int, float)) else 1.0
            if A and B:
                rules.append([set(A), set(B), float(w)])
        # silently skip malformed rows
    return rules

# =========================
# Capacity check & helpers
# =========================
def validate_shelf_capacity(item_to_shelf: Dict[str, Coord], max_per_shelf: int = MAX_ITEMS_PER_SHELF):
    """Hard capacity check for every shelf."""
    counts = defaultdict(int)
    for _, s in item_to_shelf.items():
        counts[s] += 1
    over = {s: cnt for s, cnt in counts.items() if cnt > max_per_shelf}
    if over:
        lines = [f"Shelf {s} count={cnt} > {max_per_shelf}" for s, cnt in over.items()]
        raise ValueError("Capacity exceeded:\n" + "\n".join(lines))

def random_assignment(items: List[str], shelves: List[Coord], max_per_shelf=MAX_ITEMS_PER_SHELF) -> Dict[str, Coord]:
    """Create a random feasible assignment (capacity respected)."""
    if len(items) > len(shelves)*max_per_shelf:
        raise ValueError("Items exceed total capacity.")
    shuffled = items[:]
    random.shuffle(shuffled)
    loads = {s: 0 for s in shelves}
    mapping = {}
    for it in shuffled:
        candidates = [s for s, cnt in loads.items() if cnt < max_per_shelf]
        if not candidates:
            raise ValueError("No free slots available.")
        s = random.choice(candidates)
        loads[s] += 1
        mapping[it] = s
    return mapping

def repair_capacity(mapping: Dict[str, Coord], shelves: List[Coord], max_per_shelf=MAX_ITEMS_PER_SHELF) -> Dict[str, Coord]:
    """Repair capacity violations by moving extra items to free shelves."""
    loads = defaultdict(list)  # shelf -> list of items
    for it, s in mapping.items():
        loads[s].append(it)

    # collect free slots
    free_slots = []
    for s in shelves:
        left = max_per_shelf - len(loads.get(s, []))
        free_slots.extend([s]*max(0, left))

    repaired = mapping.copy()
    for s, items in loads.items():
        extra = len(items) - max_per_shelf
        if extra > 0:
            move_list = random.sample(items, extra)
            for it in move_list:
                if not free_slots:
                    return repaired  # cannot fully repair if overfull globally
                new_s = free_slots.pop()
                repaired[it] = new_s
    return repaired

def complete_init_mapping(
    items: List[str],
    shelves: List[Coord],
    init_mapping: Dict[str, Coord],
    max_per_shelf: int = MAX_ITEMS_PER_SHELF,
    seed: int = 1234
) -> Dict[str, Coord]:
    """Complete initial mapping so every item has a shelf; enforce capacity."""
    random.seed(seed)
    mapping = dict(init_mapping) if init_mapping else {}
    loads = defaultdict(int)
    for it, s in mapping.items():
        loads[s] += 1
        if loads[s] > max_per_shelf:
            raise ValueError(f"Initial mapping exceeds capacity at shelf {s}")

    free_slots = []
    for s in shelves:
        left = max_per_shelf - loads.get(s, 0)
        free_slots.extend([s]*max(0, left))

    for it in items:
        if it not in mapping:
            if not free_slots:
                raise ValueError("Initial mapping + remaining items exceed total capacity.")
            mapping[it] = free_slots.pop()
            loads[mapping[it]] += 1
    return mapping

# =========================
# Shortest path over aisles (evaluation)
# =========================
def shortest_path_cover_shelves(
    grid: Grid,
    start: Coord,
    basket_items: List[str],
    item_to_shelf: Dict[str, Coord],
) -> Dict[str, Any]:
    """
    BFS in state-space (position, visited_mask).
    You 'collect' an item's shelf by stepping on any aisle adjacent to that shelf.
    """
    H, W = len(grid), len(grid[0])
    if not in_bounds(start[0], start[1], H, W): return dict(ok=False, reason="Start out of bounds.")
    if not is_aisle(grid, start): return dict(ok=False, reason="Start must be on an aisle (0).")

    # unique shelves to visit (deduplicate shelves if multiple items on same shelf)
    shelves_needed = []
    seen = set()
    for item in basket_items:
        if item not in item_to_shelf:
            return dict(ok=False, reason=f"Item '{item}' missing in item_to_shelf.")
        s = item_to_shelf[item]
        if s not in seen:
            seen.add(s)
            shelves_needed.append(s)

    # pickup positions for each shelf
    picks = []
    for s in shelves_needed:
        adj = any_adjacent_aisles(grid, s)
        picks.append(adj)

    # reachability check
    reachable = reachable_aisles_from_start(grid, start)
    for adj in picks:
        if not adj or not any(a in reachable for a in adj):
            return dict(ok=False, reason="Unreachable shelf via aisles.")

    # aisle cell -> bitmask
    mask_map = defaultdict(int)
    for idx, adj in enumerate(picks):
        bit = 1 << idx
        for cell in adj:
            mask_map[cell] |= bit

    full_mask = (1 << len(shelves_needed)) - 1
    start_mask = mask_map.get(start, 0)

    q = deque()
    q.append((start, start_mask))
    dist = {(start, start_mask): 0}

    while q:
        (r, c), m = q.popleft()
        if m == full_mask:
            steps = dist[((r, c), m)]
            return dict(ok=True, steps=steps, avg_steps_per_item=steps/max(len(basket_items),1), visited_shelves=shelves_needed)
        for nr, nc in neighbors4(grid, (r, c)):
            if not is_aisle(grid, (nr, nc)):
                continue
            nm = m | mask_map.get((nr, nc), 0)
            st = ((nr, nc), nm)
            if st not in dist:
                dist[st] = dist[((r, c), m)] + 1
                q.append(st)
    return dict(ok=False, reason="No feasible path found.")

def evaluate_layout(
    grid: Grid,
    start: Coord,
    basket_items: List[str],
    item_to_shelf: Dict[str, Coord],
) -> Dict[str, Any]:
    """Wrapper consistent with your previous interface."""
    validate_shelf_capacity(item_to_shelf, MAX_ITEMS_PER_SHELF)
    return shortest_path_cover_shelves(grid, start, basket_items, item_to_shelf)

# =========================
# Association-rule proximity (supports two formats)
# =========================
def manhattan(a: Coord, b: Coord) -> int:
    return abs(a[0]-b[0]) + abs(a[1]-b[1])

def _rule_iter(rules: List[Any]):
    """
    Yield (antecedent_iterable, consequent_iterable, weight or None) for each rule.
    Supported formats:
      1) [ set(A), set(B), weight ]  (your example)
      2) {'antecedent': [...], 'consequent': [...], 'support':..., 'confidence':..., 'lift':...}
    """
    for r in rules:
        if isinstance(r, (list, tuple)) and len(r) >= 2:
            A = list(next(iter([r[0]]))) if isinstance(r[0], set) else list(r[0])
            B = list(next(iter([r[1]]))) if isinstance(r[1], set) else list(r[1])
            w = float(r[2]) if len(r) >= 3 and isinstance(r[2], (int, float)) else None
            yield (A, B, w)
        elif isinstance(r, dict):
            A = list(r.get('antecedent', []))
            B = list(r.get('consequent', []))
            w = None
            yield (A, B, w)
        else:
            continue

def rule_proximity_cost(
    item_to_shelf: Dict[str, Coord],
    rules: List[Any],
    weight_mode: str = "auto",
    transform: Optional[Dict] = None
) -> float:
    """
    Weighted average 'distance' between antecedent and consequent (min pairwise).
    Lower is better (closer pairs).
    """
    num = 0.0
    den = 0.0
    for rawA, rawB, provided_w in _rule_iter(rules):
        if not rawA or not rawB:
            continue
        # compute weight
        if weight_mode == "auto" and provided_w is not None:
            w = float(provided_w)
        elif isinstance(provided_w, (int, float)):
            w = float(provided_w)
        else:
            w = 1.0

        # min Manhattan distance between any a in A and b in B
        dmin = None
        for a in rawA:
            if a not in item_to_shelf:
                continue
            for b in rawB:
                if b not in item_to_shelf:
                    continue
                d = manhattan(item_to_shelf[a], item_to_shelf[b])
                dmin = d if dmin is None else min(dmin, d)
        if dmin is None:
            continue

        # transform (sigmoid to emphasize near neighbors)
        if transform and transform.get('type') == 'sigmoid':
            k  = float(transform.get('k', 1.0))
            d0 = float(transform.get('d0', 2.0))
            dval = 1.0 / (1.0 + math.exp(-k * (dmin - d0)))  # in (0,1), closer -> smaller
        else:
            dval = float(dmin)

        num += w * dval
        den += w

    return 0.0 if den == 0.0 else num / den

# =========================
# Baseline estimation & normalized objective
# =========================
def estimate_baselines(
    items: List[str],
    shelves: List[Coord],
    grid: Grid,
    start: Coord,
    rules: List[Any],
    baskets: Optional[List[List[str]]] = None,
    samples: int = 30,
    rule_transform: Optional[Dict] = None,
) -> Tuple[float, float]:
    """
    Monte-Carlo random feasible mappings to estimate typical magnitudes
    for path-term and rule-term, used for normalization.
    """
    path_vals, rule_vals = [], []
    for _ in range(samples):
        mapping = random_assignment(items, shelves)
        mapping = repair_capacity(mapping, shelves)
        # rule term
        rv = rule_proximity_cost(mapping, rules, weight_mode="auto", transform=rule_transform)
        rule_vals.append(rv)
        # path term
        if baskets:
            s = 0.0
            for b in baskets:
                res = evaluate_layout(grid, start, b, mapping)
                s += res['steps'] if res.get('ok') else 1e6
            s /= max(len(baskets), 1)
            path_vals.append(s)

    path_mu = (sum(path_vals)/len(path_vals)) if path_vals else 1.0
    rule_mu = (sum(rule_vals)/len(rule_vals)) if rule_vals else 1.0
    return path_mu, rule_mu

def objective(
    mapping: Dict[str, Coord],
    grid: Grid,
    start: Coord,
    baskets: Optional[List[List[str]]],
    rules: List[Any],
    alpha: float = 1.0,
    beta: float  = 1.0,
    baselines: Optional[Tuple[float, float]] = None,   # (path_mu, rule_mu)
    rule_transform: Optional[Dict] = None
) -> float:
    """Normalized linear objective: alpha*path_norm + beta*rule_norm (lower is better)."""
    # rule term
    rule_term = rule_proximity_cost(mapping, rules, weight_mode="auto", transform=rule_transform)

    # path term
    path_term = 0.0
    if baskets:
        for b in baskets:
            res = evaluate_layout(grid, start, b, mapping)
            path_term += (res['steps'] if res.get('ok') else 1e6)
        path_term /= max(len(baskets), 1)

    # normalization using baselines
    if baselines:
        path_mu, rule_mu = baselines
        path_norm = path_term / (path_mu + 1e-6) if baskets else 0.0
        rule_norm = rule_term / (rule_mu + 1e-6)
    else:
        path_norm = path_term
        rule_norm = rule_term

    return alpha * path_norm + beta * rule_norm

# =========================
# GA representation & ops
# =========================
def mapping_to_gene(items: List[str], shelves: List[Coord], mapping: Dict[str, Coord]) -> List[int]:
    """Encode mapping as shelf indices per item (order of 'items')."""
    shelf_idx = {s: i for i, s in enumerate(shelves)}
    try:
        return [shelf_idx[mapping[it]] for it in items]
    except KeyError as e:
        missing = str(e).strip("'")
        raise KeyError(f"Item '{missing}' is missing in initial mapping. "
                       f"Use complete_init_mapping(...) to fill.") from e

def gene_to_mapping(items: List[str], shelves: List[Coord], gene: List[int]) -> Dict[str, Coord]:
    return {it: shelves[idx] for it, idx in zip(items, gene)}

def gene_repair_capacity(gene: List[int], shelves: List[Coord], items: List[str], max_per_shelf=MAX_ITEMS_PER_SHELF) -> List[int]:
    """Repair gene to satisfy shelf capacity."""
    loads = defaultdict(list)  # shelf_idx -> list of item indices
    for i, sidx in enumerate(gene):
        loads[sidx].append(i)

    # free slots (as shelf indices)
    free_slots = []
    for sidx in range(len(shelves)):
        left = max_per_shelf - len(loads.get(sidx, []))
        free_slots.extend([sidx]*max(0, left))

    repaired = gene[:]
    for sidx, lst in loads.items():
        extra = len(lst) - max_per_shelf
        if extra > 0:
            move_ids = random.sample(lst, extra)
            for mid in move_ids:
                if not free_slots:
                    return repaired
                new_sidx = free_slots.pop()
                repaired[mid] = new_sidx
    return repaired

def init_population(pop_size: int, items: List[str], shelves: List[Coord], init_mapping: Dict[str, Coord]) -> List[List[int]]:
    """Include completed initial mapping as seed; fill rest with random ones."""
    pop = []
    if init_mapping:
        completed = complete_init_mapping(items, shelves, init_mapping, MAX_ITEMS_PER_SHELF)
        g0 = mapping_to_gene(items, shelves, completed)
        g0 = gene_repair_capacity(g0, shelves, items)
        pop.append(g0)
    while len(pop) < pop_size:
        rd = random_assignment(items, shelves)
        g = mapping_to_gene(items, shelves, rd)
        g = gene_repair_capacity(g, shelves, items)
        pop.append(g)
    return pop

def tournament_select(pop: List[List[int]], fitness: List[float], k: int = 3) -> List[int]:
    """Lower fitness is better."""
    cand = random.sample(range(len(pop)), k)
    cand.sort(key=lambda i: fitness[i])
    return pop[cand[0]][:]

def crossover(p1: List[int], p2: List[int], cx_rate: float = 0.8) -> Tuple[List[int], List[int]]:
    if random.random() > cx_rate or len(p1) < 2:
        return p1[:], p2[:]
    cut = random.randint(1, len(p1)-1)
    c1 = p1[:cut] + p2[cut:]
    c2 = p2[:cut] + p1[cut:]
    return c1, c2

def mutate(g: List[int], shelves: List[Coord], mut_rate: float = 0.2):
    if random.random() > mut_rate:
        return
    i = random.randrange(len(g))
    g[i] = random.randrange(len(shelves))


def _extract_weighted_pairs_from_rules(
    rules: List[Any],
    item_names: List[str],
    top_k_pairs: int = 300,
    weight_threshold: Optional[float] = None
) -> List[Tuple[str, str, float]]:
    """
    Parse rules into (a, b, weight) pairs.
    - Supports your triplet format: [ set(A), set(B), weight ]
    - Ignores items not in item_names.
    - Deduplicates unordered pairs: (a,b) == (b,a).
    """
    name_set = set(item_names)
    seen = set()
    pairs: List[Tuple[str, str, float]] = []
    for r in rules:
        if isinstance(r, (list, tuple)) and len(r) >= 2:
            A = list(r[0]) if isinstance(r[0], (set, list, tuple)) else [r[0]]
            B = list(r[1]) if isinstance(r[1], (set, list, tuple)) else [r[1]]
            w = float(r[2]) if len(r) >= 3 and isinstance(r[2], (int, float)) else 1.0
        elif isinstance(r, dict):
            A = list(r.get('antecedent', []))
            B = list(r.get('consequent', []))
            # If no explicit weight, fall back to 1.0
            w = float(r.get('lift', 1.0)) * float(r.get('support', 1.0)) if ('lift' in r and 'support' in r) else float(r.get('confidence', 1.0))
        else:
            continue

        if weight_threshold is not None and w < weight_threshold:
            continue

        for a in A:
            if a not in name_set:
                continue
            for b in B:
                if b not in name_set or a == b:
                    continue
                key = tuple(sorted((a, b)))
                if key in seen:
                    continue
                seen.add(key)
                pairs.append((key[0], key[1], w))

    # sort by weight desc, keep top-k
    pairs.sort(key=lambda x: x[2], reverse=True)
    if top_k_pairs is not None and top_k_pairs > 0:
        pairs = pairs[:top_k_pairs]
    return pairs

def _shelf_neighbors(shelves: List[Coord]) -> Dict[Coord, List[Coord]]:
    """
    Precompute adjacency among shelves (Manhattan distance == 1).
    """
    shelfs = list(shelves)
    nbrs = {s: [] for s in shelfs}
    sset = set(shelfs)
    for r, c in shelfs:
        for dr, dc in [(1,0),(-1,0),(0,1),(0,-1)]:
            t = (r+dr, c+dc)
            if t in sset:
                nbrs[(r,c)].append(t)
    return nbrs

def make_initial_mapping(
    item_names: List[str],
    shelves: List[Coord],
    rules: Optional[List[Any]] = None,
    *,
    mode: str = "random",             # "random" or "rule_aware"
    seed: Optional[int] = 42,
    max_per_shelf: int = MAX_ITEMS_PER_SHELF,
    rule_top_k_pairs: int = 300,
    rule_weight_threshold: Optional[float] = None,
    co_locate_prob: float = 0.85      # prob. to force pair to same/neighbor shelf when possible
) -> Dict[str, Coord]:
    """
    Build an initial placement mapping:
      - mode="random": pure random assignment (capacity respected).
      - mode="rule_aware": try to place strong rule pairs on same/adjacent shelves, then fill rest randomly.
    Returns: dict[item_name] -> shelf_coord
    Raises: ValueError if total capacity is insufficient.
    """
    if seed is not None:
        random.seed(seed)

    n_items = len(item_names)
    capacity = len(shelves) * max_per_shelf
    if n_items > capacity:
        raise ValueError(f"Not enough capacity: {capacity} < {n_items}")

    # Fast path: pure random using your existing helper
    if mode == "random" or not rules:
        mapping = random_assignment(item_names, shelves, max_per_shelf)
        mapping = repair_capacity(mapping, shelves, max_per_shelf)
        validate_shelf_capacity(mapping, max_per_shelf)
        return mapping

    # Rule-aware seeding
    pairs = _extract_weighted_pairs_from_rules(
        rules, item_names, top_k_pairs=rule_top_k_pairs, weight_threshold=rule_weight_threshold
    )
    neighbors = _shelf_neighbors(shelves)

    # track free slots on each shelf
    loads: Dict[Coord, int] = {s: 0 for s in shelves}
    mapping: Dict[str, Coord] = {}
    unplaced = set(item_names)

    def shelves_with_free(k: int = 1) -> List[Coord]:
        return [s for s, cnt in loads.items() if cnt <= max_per_shelf - k]

    # 1) place high-weight pairs first
    for a, b, w in pairs:
        if a not in unplaced and b not in unplaced:
            continue  # both already placed by previous decisions
        if random.random() > co_locate_prob:
            continue  # occasionally skip to keep diversity

        # cases:
        # (i) both unplaced -> try same shelf if >=2 slots, else neighbor shelves each >=1 slot
        if a in unplaced and b in unplaced:
            # same shelf
            same_candidates = shelves_with_free(k=2)
            random.shuffle(same_candidates)
            placed = False
            for s in same_candidates:
                mapping[a] = s
                mapping[b] = s
                loads[s] += 2
                unplaced.remove(a); unplaced.remove(b)
                placed = True
                break
            if placed:
                continue
            # neighbor shelves
            placed = False
            # iterate shelves that have >=1 slot, try neighbor also >=1
            one_candidates = shelves_with_free(k=1)
            random.shuffle(one_candidates)
            for s in one_candidates:
                nbs = [t for t in neighbors[s] if loads[t] <= max_per_shelf - 1]
                if not nbs:
                    continue
                t = random.choice(nbs)
                mapping[a] = s
                mapping[b] = t
                loads[s] += 1; loads[t] += 1
                unplaced.remove(a); unplaced.remove(b)
                placed = True
                break
            if placed:
                continue
            # fallback: do nothing here; will fill randomly later

        # (ii) one placed, one unplaced -> try same shelf or neighbor of the placed one
        elif a in unplaced or b in unplaced:
            placed_item = b if a in unplaced else a
            free_item   = a if a in unplaced else b
            s = mapping.get(placed_item, None)
            if s is not None:
                # same shelf
                if loads[s] <= max_per_shelf - 1:
                    mapping[free_item] = s
                    loads[s] += 1
                    unplaced.discard(free_item)
                    continue
                # neighbor shelf
                nbs = [t for t in neighbors[s] if loads[t] <= max_per_shelf - 1]
                if nbs:
                    t = random.choice(nbs)
                    mapping[free_item] = t
                    loads[t] += 1
                    unplaced.discard(free_item)
                    continue
            # else placed_item not in mapping (shouldn't happen), fall through

    # 2) randomly fill the remaining items
    # build a flat list of free slots: each shelf repeated by remaining slots
    free_slots: List[Coord] = []
    for s, cnt in loads.items():
        left = max_per_shelf - cnt
        if left > 0:
            free_slots.extend([s] * left)
    random.shuffle(free_slots)

    if len(unplaced) > len(free_slots):
        raise ValueError("Not enough free slots after rule-aware seeding.")

    for it in list(unplaced):
        s = free_slots.pop()
        mapping[it] = s
        loads[s] += 1
        unplaced.remove(it)

    # Final safety
    mapping = repair_capacity(mapping, shelves, max_per_shelf)
    validate_shelf_capacity(mapping, max_per_shelf)
    return mapping

def print_mapping_table(item_to_shelf: Dict[str, Tuple[int,int]], title: str = "Item → Shelf"):
    rows = sorted(item_to_shelf.items(), key=lambda kv: (kv[1][0], kv[1][1], kv[0]))
    name_w = max(5, min(40, max(len(k) for k, _ in rows)))
    print(f"\n=== {title} ===")
    print(f"{'Item':<{name_w}}  {'Row':>3}  {'Col':>3}")
    print("-" * (name_w + 10))
    for name, (r, c) in rows:
        print(f"{name:<{name_w}}  {r:>3}  {c:>3}")


# =========================
# GA main
# =========================
def run_ga_optimize(
    GRID_H: int,
    GRID_W: int,
    SHELVES: List[Coord],
    START: Coord,
    ITEM_NAMES: List[str],
    INIT_ITEM_TO_SHELF: Dict[str, Coord],
    RULES: List[Any],
    BASKETS: Optional[List[List[str]]] = None,
    pop_size: int = 30,
    generations: int = 100,
    cx_rate: float = 0.8,
    mut_rate: float = 0.3,
    elite: int = 2,
    alpha: float = 1.0,         # weight for normalized path term
    beta: float  = 1.0,         # weight for normalized rule term
    use_sigmoid_rule: bool = True,
    baseline_samples: int = 30
) -> Dict[str, Coord]:
    """
    Run GA to optimize item placement under capacity constraints.
    Objective = alpha * normalized_path + beta * normalized_rule.
    """
    random.seed()

    grid = build_grid_from_shelves(GRID_H, GRID_W, SHELVES)
    rule_tf = {'type':'sigmoid', 'k':1.5, 'd0':2.0} if use_sigmoid_rule else None

    # Estimate baselines for normalization
    baselines = estimate_baselines(
        items=ITEM_NAMES, shelves=SHELVES, grid=grid, start=START,
        rules=RULES, baskets=BASKETS, samples=baseline_samples, rule_transform=rule_tf
    )

    # Initialize population
    pop = init_population(pop_size, ITEM_NAMES, SHELVES, INIT_ITEM_TO_SHELF)

    def fitness_of(gene: List[int]) -> float:
        mapping = gene_to_mapping(ITEM_NAMES, SHELVES, gene)
        mapping = repair_capacity(mapping, SHELVES)
        try:
            validate_shelf_capacity(mapping, MAX_ITEMS_PER_SHELF)
        except:
            return 1e12
        return objective(
            mapping, grid, START, BASKETS, RULES,
            alpha=alpha, beta=beta,
            baselines=baselines,
            rule_transform=rule_tf
        )

    fitness = [fitness_of(g) for g in pop]
    for _ in range(generations):
        # elitism
        idx_sorted = sorted(range(len(pop)), key=lambda i: fitness[i])
        new_pop = [pop[i][:] for i in idx_sorted[:elite]]

        # offspring
        while len(new_pop) < pop_size:
            p1 = tournament_select(pop, fitness, k=3)
            p2 = tournament_select(pop, fitness, k=3)
            c1, c2 = crossover(p1, p2, cx_rate)
            mutate(c1, SHELVES, mut_rate)
            mutate(c2, SHELVES, mut_rate)
            c1 = gene_repair_capacity(c1, SHELVES, ITEM_NAMES)
            c2 = gene_repair_capacity(c2, SHELVES, ITEM_NAMES)
            new_pop.append(c1)
            if len(new_pop) < pop_size:
                new_pop.append(c2)

        pop = new_pop
        fitness = [fitness_of(g) for g in pop]

    # Best mapping
    best_idx = min(range(len(pop)), key=lambda i: fitness[i])
    best_gene = pop[best_idx]
    best_mapping = gene_to_mapping(ITEM_NAMES, SHELVES, best_gene)
    best_mapping = repair_capacity(best_mapping, SHELVES)
    validate_shelf_capacity(best_mapping, MAX_ITEMS_PER_SHELF)
    return best_mapping

# =========================
# ======== CONFIG =========
# =========================
if __name__ == "__main__":
    # Grid & shelves (manual)
    GRID_H, GRID_W = 10, 10
    SHELVES: List[Coord] = [
        (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (3, 1), (3, 2), (3, 3), (3, 4),
        (3, 5), (3, 6), (3, 7), (3, 8), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (5, 8),
        (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7), (7, 8)
    ]
    START: Coord = (0, 0)

    # Items (example; replace with your full list)
    ITEM_NAMES = [
    'almonds','antioxydant juice','asparagus','avocado','babies food','bacon',
    'barbecue sauce','black tea','blueberries','body spray','bramble','brownies',
    'bug spray','burger sauce','burgers','butter','cake','candy bars','carrots',
    'cauliflower','cereals','champagne','chicken','chili','chocolate',
    'chocolate bread','chutney','cider','clothes accessories','cookies',
    'cooking oil','corn','cottage cheese','cream','dessert wine','eggplant','eggs',
    'energy bar','energy drink','escalope','extra dark chocolate','flax seed',
    'french fries','french wine','fresh bread','fresh tuna','fromage blanc',
    'frozen smoothie','frozen vegetables','gluten free bar','grated cheese',
    'green beans','green grapes','green tea','ground beef','gums','ham',
    'hand protein bar','herb & pepper','honey','hot dogs','ketchup','light cream',
    'light mayo','low fat yogurt','magazines','mashed potato','mayonnaise',
    'meatballs','melons','milk','mineral water','mint','mint green tea','muffins',
    'mushroom cream sauce','napkins','nonfat milk','oatmeal','oil','olive oil',
    'pancakes','parmesan cheese','pasta','pepper','pet food','pickles',
    'protein bar','red wine','rice','salad','salmon','salt','sandwich','shallot',
    'shampoo','shrimp','soda','soup','spaghetti','sparkling water','spinach',
    'strawberries','strong cheese','tea','tomato juice','tomato sauce','tomatoes',
    'toothpaste','turkey','vegetables mix','water spray','white wine',
    'whole weat flour','whole wheat pasta','whole wheat rice','yams','yogurt cake',
    'zucchini'
    ]

    # ---- Load rules from JSON (same folder as the notebook/script) ----
    #extract_rules("GNN")
    RULES_JSON_PATH = "results/rules.json"
    RULES = load_rules_from_json(RULES_JSON_PATH)

    BASKETS = ITEM_NAMES
    INIT_ITEM_TO_SHELF = make_initial_mapping(
    ITEM_NAMES, SHELVES, rules=RULES, mode="rule_aware", seed=42
    )
    # Run GA with normalized objective
    best_mapping = run_ga_optimize(
        GRID_H=GRID_H, GRID_W=GRID_W,
        SHELVES=SHELVES, START=START,
        ITEM_NAMES=ITEM_NAMES,
        INIT_ITEM_TO_SHELF=INIT_ITEM_TO_SHELF,
        RULES=RULES, BASKETS=BASKETS,
        pop_size=40, generations=80,
        cx_rate=0.8, mut_rate=0.2, elite=2,
        alpha=1.0, beta=1.0,
        use_sigmoid_rule=True,
        baseline_samples=30
    )
    print_mapping_table(best_mapping, title="Optimized placement")
    # ---- Save optimized mapping to file ----
    OUTPUT_PATH = "optimized_mapping.json"
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
      json.dump({item: [int(rc[0]), int(rc[1])] for item, rc in best_mapping.items()},
              f, ensure_ascii=False, indent=2)
    print(f"\nSaved optimized mapping to: {OUTPUT_PATH}")